# -*- coding: utf-8 -*-
"""2409087MLcw.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/134IhMrDISTMnbC0wOxDGFlWD_ubdm_z5
"""

import pandas as pd

# Load the dataset,
data = pd.read_csv("/content/bank-additional-full.csv", sep = ';')
display(data.head()) # Display the very first few rows of the dataset.

# Read the metadata file,
with open('/content/bank-additional-names.txt', 'r') as file:
    column_descriptions = file.readlines()

# Display the very first few lines of the metadata,
print("\nColumn Descriptions:")
with open('bank-additional-names.txt', 'r') as file:
    for line in file:
        print(line.strip())

# Get a summary of the dataset,
print("\nDataset Summary:")
print(data.info())

# Check for missing values,
print("\nMissing Values:")
print(data.isnull().sum())

# Check for duplicate values,
print("\nDuplicate Values:")
print(data.duplicated().sum())

# Statistical summary,
print("\nStatistical Summary:")
print(data.describe())

# Identify Target Variable
target_column = 'y'
if target_column in data.columns:
    print(f"\nTarget Variable ('{target_column}') Distribution:")
    print(data[target_column].value_counts())
else:
    print(f"Target column '{target_column}' not found in dataset.")

# Identify Categorical and Numerical Features
categorical_features = data.select_dtypes(include=['object']).columns
numerical_features = data.select_dtypes(include=['int64', 'float64']).columns

print("\nCategorical Features:")
print(list(categorical_features))

print("\nNumerical Features:")
print(list(numerical_features))

# Unique Values in Sample Categorical Column
sample_categorical = 'job'
if sample_categorical in data.columns:
    print(f"\nUnique Values in '{sample_categorical}':")
    print(data[sample_categorical].unique())

# Detailed Statistics of Sample Numerical Column
sample_numerical = 'age'
if sample_numerical in data.columns:
    print(f"\nStatistics for '{sample_numerical}':")
    print(data[sample_numerical].describe())

#Pre Processing
from scipy.stats import zscore # Import zscore

# Import StandardScaler
from sklearn.preprocessing import StandardScaler, LabelEncoder

# Handling Missing Values

# Fill missing numerical values with the mean
data[numerical_features] = data[numerical_features].fillna(data[numerical_features].mean())
# Fill missing categorical values with "Unknown"
data[categorical_features] = data[categorical_features].fillna("Unknown")

# Encoding Categorical Features - One-Hot Encoding
data_onehot = pd.get_dummies(data, columns=categorical_features, drop_first=True)

# Label Encoding (for ordinal features)
data_label = data.copy()
for col in categorical_features:
    label_encoder = LabelEncoder()
    data_label[col] = label_encoder.fit_transform(data_label[col])

# Handling Outliers (using Z-score)
z_scores = zscore(data[numerical_features])
data = data[(z_scores < 3).all(axis=1)]  # Remove rows with Z-score > 3

# Feature Scaling
scaler = StandardScaler()
data[numerical_features] = scaler.fit_transform(data[numerical_features])

# Save preprocessed data to a new CSV file
data.to_csv('preprocessed.csv', index=False) # index=False prevents saving row index

# Print some info to check preprocessing:
print("\nPreprocessed Data:")
print(data.head())  # Print the first few rows

print("\nData Info after Preprocessing:")
print(data.info())  # Check data types and missing values

print("\nDescriptive Statistics after Preprocessing:")
print(data.describe())  # Check for scaling and outlier removal effect

# Print specific columns to check encoding:
print("\nEncoded Categorical Feature (e.g., job):")
print(data['job'].head())  # data_label['job'] for label encoding

print("\nOne-Hot Encoded Features (first few rows):")
print(data_onehot.iloc[:5, -10:])  # Print last 10 one-hot encoded columns

# Split data into training and testing sets
from sklearn.model_selection import train_test_split

# Assuming 'y' is the target variable column
X = data.drop('y', axis=1)  # Features
y = data['y']  # Target variable

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # 80% train, 20% test

# X_train: Training features
# X_test: Testing features
# y_train: Training target variable
# y_test: Testing target variable

print("\nShape of training data:", X_train.shape)
print("Shape of testing data:", X_test.shape)

# Check target variable balance
target_distribution = y.value_counts()
print("\nTarget Variable Distribution:")
print(target_distribution)

class_proportions = target_distribution / len(y)
print("\nClass Proportions:")
print(class_proportions)

imbalance_threshold = 0.1
is_balanced = (class_proportions.min() > imbalance_threshold)

if is_balanced:
    print("\nThe target variable is balanced.")
else:
    print("\nThe target variable is imbalanced.")

!pip install imblearn

from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from imblearn.over_sampling import SMOTE

# Identify categorical features
categorical_features = X_train.select_dtypes(include=['object']).columns

# Apply one-hot encoding to both training and testing data
X_train_encoded = pd.get_dummies(X_train, columns=categorical_features, drop_first=True)
X_test_encoded = pd.get_dummies(X_test, columns=categorical_features, drop_first=True)

# Align the columns of X_test_encoded to match X_train_encoded
X_test_encoded = X_test_encoded.reindex(columns=X_train_encoded.columns, fill_value=0)

# Ensure SMOTE is applied only to the training data after encoding
smote = SMOTE(random_state=42)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train_encoded, y_train)

# Check the balance of the target variable after balancing
print("\nTarget Variable Distribution After Balancing:")
print(y_train_balanced.value_counts())

# Random Forest Model
import pandas as pd
import joblib
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Identify categorical features
categorical_features = X_train.select_dtypes(include=['object']).columns

# Apply one-hot encoding to both training and testing data
X_train_encoded = pd.get_dummies(X_train, columns=categorical_features, drop_first=True)
X_test_encoded = pd.get_dummies(X_test, columns=categorical_features, drop_first=True)

# Align the columns of X_test_encoded to match X_train_encoded
X_test_encoded = X_test_encoded.reindex(columns=X_train_encoded.columns, fill_value=0)

# Ensure SMOTE is applied only to the training data after encoding
smote = SMOTE(random_state=42)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train_encoded, y_train)

# Train the Random Forest model
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train_balanced, y_train_balanced)
joblib.dump(rf_model, 'rf_model.pkl')  # Save after training

# Make Predictions on the test data
y_pred = rf_model.predict(X_test_encoded)

# Evaluate the model performance
print("\nRandom Forest Model Performance:")
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# Neural Network Model
import numpy as np
import random
import tensorflow as tf
import pandas as pd
from sklearn.model_selection import train_test_split

# Set random seeds for reproducibility
random.seed(42)
np.random.seed(42)
tf.random.set_seed(42)

X = data.drop('y', axis=1)  # Features
y = data['y']  # Target variable

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# One-Hot Encoding Categorical Features
categorical_features = X.select_dtypes(include=['object']).columns

# Apply one-hot encoding to the training and test data
X_train_encoded = pd.get_dummies(X_train, columns=categorical_features, drop_first=True)
X_test_encoded = pd.get_dummies(X_test, columns=categorical_features, drop_first=True)

# Align the columns of X_test_encoded to match X_train_encoded (in case some columns are missing)
X_test_encoded = X_test_encoded.reindex(columns=X_train_encoded.columns, fill_value=0)

# One-Hot Encoding the Target Variable
y_train_encoded = pd.get_dummies(y_train)
y_test_encoded = pd.get_dummies(y_test)

# Convert data to NumPy arrays (for TensorFlow compatibility)
X_train_encoded = X_train_encoded.values
X_test_encoded = X_test_encoded.values
y_train_encoded = y_train_encoded.values
y_test_encoded = y_test_encoded.values

# Convert data to TensorFlow tensors
X_train_tensor = tf.convert_to_tensor(X_train_encoded, dtype=tf.float32)
X_test_tensor = tf.convert_to_tensor(X_test_encoded, dtype=tf.float32)
y_train_tensor = tf.convert_to_tensor(y_train_encoded, dtype=tf.float32)
y_test_tensor = tf.convert_to_tensor(y_test_encoded, dtype=tf.float32)

# Define the Neural Network Model
def create_model():
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train_tensor.shape[1],)),
        tf.keras.layers.Dropout(0.3),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dropout(0.3),
        tf.keras.layers.Dense(y_train_tensor.shape[1], activation='softmax')  # Output layer
    ])
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# Create and Train the Model
model = create_model()

# Train the model and display epochs progress
history = model.fit(X_train_tensor, y_train_tensor,
                    epochs=20,
                    batch_size=32,
                    validation_data=(X_test_tensor, y_test_tensor),
                    verbose=1)  # Set to 1 for epoch progress

#Save the model
model.save('nn_model.h5')  # Save after training

# Evaluate the Model
test_loss, test_accuracy = model.evaluate(X_test_tensor, y_test_tensor, verbose=1)

# Print the Accuracy
print(f"\nTest Accuracy: {test_accuracy:.4f}")

#Comparing two models

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import joblib
from sklearn.preprocessing import LabelEncoder

# Reloading Random Forest Model
try:
    rf_model = joblib.load('rf_model.pkl')  # Reloading the Random Forest model
    print("Random Forest model loaded successfully.")
except FileNotFoundError:
    print("Random Forest model not found. Please ensure it was trained and saved.")

# Reloading Neural Network Model
try:
    nn_model = tf.keras.models.load_model('nn_model.h5')  # Reloading the Neural Network model
    print("Neural Network model loaded successfully.")
except IOError:
    print("Neural Network model not found. Please ensure it was trained and saved.")

# Preprocessing Data
le = LabelEncoder()
y_test_numeric = le.fit_transform(y_test)  # Converts 'no' to 0 and 'yes' to 1

# Evaluate the Random Forest Model
if 'rf_model' in locals():
    # Ensure X_test_encoded matches the trained model's features
    try:
        y_pred_rf = rf_model.predict(X_test_encoded)
        y_pred_rf_numeric = le.transform(y_pred_rf)
        rf_probabilities = rf_model.predict_proba(X_test_encoded)

        # Random Forest Metrics
        rf_accuracy = accuracy_score(y_test_numeric, y_pred_rf_numeric)
        rf_precision = precision_score(y_test_numeric, y_pred_rf_numeric)
        rf_recall = recall_score(y_test_numeric, y_pred_rf_numeric)
        rf_f1 = f1_score(y_test_numeric, y_pred_rf_numeric)
        rf_roc_auc = roc_auc_score(y_test_numeric, rf_probabilities[:, 1])
    except Exception as e:
        print(f"Error during Random Forest evaluation: {e}")

# Evaluate the Neural Network Model
if 'nn_model' in locals():
    try:
        y_pred_nn = nn_model.predict(X_test_tensor)
        y_pred_nn_classes = np.argmax(y_pred_nn, axis=1)

        # Neural Network Metrics
        nn_accuracy = accuracy_score(y_test_numeric, y_pred_nn_classes)
        nn_precision = precision_score(y_test_numeric, y_pred_nn_classes)
        nn_recall = recall_score(y_test_numeric, y_pred_nn_classes)
        nn_f1 = f1_score(y_test_numeric, y_pred_nn_classes)
        nn_roc_auc = roc_auc_score(y_test_numeric, y_pred_nn[:, 1])
    except Exception as e:
        print(f"Error during Neural Network evaluation: {e}")

# Print the Comparison Table

print("\n--- Model Comparison ---")
print("Metric\t\tRandom Forest\tNeural Network")
if 'rf_model' in locals():
    print(f"Accuracy\t\t{rf_accuracy:.4f}\t\t", end='')
if 'nn_model' in locals():
    print(f"{nn_accuracy:.4f}")
if 'rf_model' in locals():
    print(f"Precision\t\t{rf_precision:.4f}\t\t", end='')
if 'nn_model' in locals():
    print(f"{nn_precision:.4f}")
if 'rf_model' in locals():
    print(f"Recall\t\t\t{rf_recall:.4f}\t\t", end='')
if 'nn_model' in locals():
    print(f"{nn_recall:.4f}")
if 'rf_model' in locals():
    print(f"F1-score\t\t{rf_f1:.4f}\t\t", end='')
if 'nn_model' in locals():
    print(f"{nn_f1:.4f}")
if 'rf_model' in locals():
    print(f"ROC-AUC\t\t\t{rf_roc_auc:.4f}\t\t", end='')
if 'nn_model' in locals():
    print(f"{nn_roc_auc:.4f}")

# Plotting the Bar Graph for Comparison
rf_metrics = [rf_accuracy, rf_precision, rf_recall, rf_f1, rf_roc_auc] if 'rf_model' in locals() else []
nn_metrics = [nn_accuracy, nn_precision, nn_recall, nn_f1, nn_roc_auc] if 'nn_model' in locals() else []
metric_labels = ['Accuracy', 'Precision', 'Recall', 'F1-score', 'ROC-AUC']

x = np.arange(len(metric_labels))
width = 0.35

fig, ax = plt.subplots()
if rf_metrics:
    rects1 = ax.bar(x - width/2, rf_metrics, width, label='Random Forest', color='purple')
if nn_metrics:
    rects2 = ax.bar(x + width/2, nn_metrics, width, label='Neural Network', color='pink')

ax.set_ylabel('Score')
ax.set_title('Model Comparison')
ax.set_xticks(x)
ax.set_xticklabels(metric_labels)
ax.legend()

def autolabel(rects):
    for rect in rects:
        height = rect.get_height()
        ax.annotate(f'{height:.4f}',
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 3),
                    textcoords="offset points",
                    ha='center', va='bottom')

print("\n\n\nVisualization of accuracies on each model:")

if rf_metrics:
    autolabel(rects1)
if nn_metrics:
    autolabel(rects2)

fig.tight_layout()
plt.show()